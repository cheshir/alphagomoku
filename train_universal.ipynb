{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# AlphaGomoku Training - Universal Notebook\n\nWorks on:\n- ‚úì Google Colab\n- ‚úì Vast.ai\n- ‚úì RunPod\n- ‚úì Lambda Labs\n- ‚úì AWS/GCP/Azure VMs\n- ‚úì Local Jupyter\n\n**Training Modes:**\n- üñ•Ô∏è **Single-machine**: Traditional training (self-play + training on same machine)\n- üåê **Distributed**: Training worker (pulls games from Redis queue)\n\n**Model:** Medium preset = 5.04M params (30 blocks √ó 192 channels)\n\n**Recommended for Distributed Training:** T4 (16GB), RTX 4090 (24GB), or A100 (40GB)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Choose Training Mode\n\n**Select your training mode:**\n- **Single-machine**: Run self-play and training on the same machine (traditional)\n- **Distributed**: Run as training worker, pulling games from Redis queue\n\nSet `TRAINING_MODE` below:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# CONFIGURATION - CHOOSE YOUR MODE\n# ============================================\n\n# Choose mode: \"single\" or \"distributed\"\nTRAINING_MODE = \"single\"  # Change to \"distributed\" for queue-based training\n\n# For distributed mode, set Redis URL:\nREDIS_URL = \"redis://:YOUR_PASSWORD@REDIS_DOMAIN:6379/0\"\n\n# ============================================\n\nimport os\nimport sys\nimport platform\n\n# Detect environment\nIS_COLAB = 'google.colab' in sys.modules\nIS_KAGGLE = 'kaggle' in os.environ.get('KAGGLE_URL_BASE', '')\n\nprint(f\"Platform: {platform.system()}\")\nprint(f\"Python: {sys.version}\")\nprint(f\"Environment: {'Colab' if IS_COLAB else 'Kaggle' if IS_KAGGLE else 'Standard VM/Local'}\")\nprint(f\"Working directory: {os.getcwd()}\")\nprint(f\"\\n{'='*50}\")\nprint(f\"Training Mode: {TRAINING_MODE.upper()}\")\nprint(f\"{'='*50}\")\n\nif TRAINING_MODE == \"distributed\":\n    print(f\"\\n‚úì Will run as TRAINING WORKER\")\n    print(f\"  Pulls games from: {REDIS_URL.split('@')[1] if '@' in REDIS_URL else REDIS_URL}\")\n    print(f\"  Publishes trained models back to queue\")\nelif TRAINING_MODE == \"single\":\n    print(f\"\\n‚úì Will run SINGLE-MACHINE training\")\n    print(f\"  Self-play + training on this machine\")\nelse:\n    raise ValueError(f\"Invalid TRAINING_MODE: {TRAINING_MODE}. Must be 'single' or 'distributed'\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Check GPU"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"GPU Memory: {gpu_memory_gb:.1f} GB\")\n",
    "    \n",
    "    # Recommend batch size\n",
    "    if gpu_memory_gb >= 32:\n",
    "        print(\"‚úì Recommended batch size: 2048\")\n",
    "    elif gpu_memory_gb >= 20:\n",
    "        print(\"‚úì Recommended batch size: 1024\")\n",
    "    elif gpu_memory_gb >= 12:\n",
    "        print(\"‚úì Recommended batch size: 512\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  GPU has limited memory, will use checkpointing\")\n",
    "else:\n",
    "    print(\"‚ùå No GPU detected! Training will be very slow.\")\n",
    "    print(\"   For Colab: Runtime ‚Üí Change runtime type ‚Üí GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup Storage (Platform-specific)\n",
    "\n",
    "Choose the appropriate cell based on your platform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === GOOGLE COLAB ONLY ===\n",
    "# Uncomment and run if using Colab\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# PROJECT_DIR = '/content/drive/MyDrive/alphagomoku'\n",
    "# WORK_DIR = '/content/alphagomoku'\n",
    "# os.makedirs(PROJECT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === VAST.AI / RUNPOD / OTHER VMs ===\n",
    "# Use local storage (usually faster than network storage)\n",
    "\n",
    "PROJECT_DIR = os.path.expanduser('~/alphagomoku')\n",
    "WORK_DIR = PROJECT_DIR\n",
    "os.makedirs(PROJECT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Project directory: {PROJECT_DIR}\")\n",
    "print(f\"Working directory: {WORK_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Get Code\n",
    "\n",
    "Choose one method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# METHOD 1: Clone from Git (Most Common)\n!git clone https://github.com/cheshir/alphagomoku.git {WORK_DIR}\n%cd {WORK_DIR}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHOD 2: If code already exists (Vast.ai with persistent storage)\n",
    "# Just navigate to it\n",
    "%cd {WORK_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHOD 3: Upload code manually (for small updates)\n",
    "# Uncomment if needed\n",
    "\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()  # Upload zip file\n",
    "# !unzip -o alphagomoku.zip -d {WORK_DIR}\n",
    "# %cd {WORK_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies\ntry:\n    import lmdb\n    import psutil\n    if TRAINING_MODE == \"distributed\":\n        import redis\n    print(\"‚úì Dependencies already installed\")\nexcept ImportError:\n    print(\"Installing dependencies...\")\n    !pip install -q numpy tqdm matplotlib lmdb psutil\n    \n    if TRAINING_MODE == \"distributed\":\n        print(\"Installing Redis client for distributed training...\")\n        !pip install -q redis\n    \n    print(\"‚úì Dependencies installed\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Training Configuration\n\nConfiguration depends on your chosen mode:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if TRAINING_MODE == \"single\":\n    # Single-machine training configuration\n    CONFIG = {\n        'epochs': 200,\n        'selfplay_games': 200,\n        'mcts_simulations': 150,\n        'parallel_workers': 4,\n        'lr': 1e-3,\n        'min_lr': 5e-4,\n        'difficulty': 'medium',\n        \n        # Paths\n        'checkpoint_dir': f'{PROJECT_DIR}/checkpoints',\n        'data_dir': f'{PROJECT_DIR}/data',\n        \n        # Device auto-configured\n        'device': 'auto',\n    }\n    \nelif TRAINING_MODE == \"distributed\":\n    # Distributed training worker configuration (GPU-optimized)\n    CONFIG = {\n        'redis_url': REDIS_URL,\n        'model_preset': 'medium',  # 5.04M params\n        'batch_size': 1024,        # Large batch for GPU efficiency\n        'min_batches_for_training': 50,  # Train when 50+ games available\n        'publish_frequency': 5,    # Publish model every 5 training iterations\n        'device': 'cuda',          # Force CUDA for cloud GPU\n        'lr': 1e-3,\n        'min_lr': 5e-4,\n        \n        # Paths\n        'checkpoint_dir': f'{PROJECT_DIR}/checkpoints',\n    }\n\n# Create directories\nos.makedirs(CONFIG['checkpoint_dir'], exist_ok=True)\nif TRAINING_MODE == \"single\":\n    os.makedirs(CONFIG['data_dir'], exist_ok=True)\n\nprint(f\"\\n{'='*50}\")\nprint(f\"Configuration ({TRAINING_MODE.upper()} mode):\")\nprint(f\"{'='*50}\")\nfor key, value in CONFIG.items():\n    if 'password' not in key.lower():  # Don't print passwords\n        print(f\"  {key}: {value}\")\n\nprint(f\"\\n‚úì Ready to train!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Start Training\n\n### Single-machine Mode:\n- Self-play generates games\n- Neural network trains on those games\n- All on this machine\n\n### Distributed Mode:\n- This machine acts as **training worker**\n- Pulls games from Redis queue (generated by Mac/other workers)\n- Trains neural network on GPU\n- Publishes trained models back to queue for workers to use"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if TRAINING_MODE == \"single\":\n    # Run traditional single-machine training\n    !python scripts/train.py \\\n        --epochs {CONFIG['epochs']} \\\n        --selfplay-games {CONFIG['selfplay_games']} \\\n        --mcts-simulations {CONFIG['mcts_simulations']} \\\n        --parallel-workers {CONFIG['parallel_workers']} \\\n        --lr {CONFIG['lr']} \\\n        --min-lr {CONFIG['min_lr']} \\\n        --warmup-epochs 0 \\\n        --lr-schedule cosine \\\n        --difficulty {CONFIG['difficulty']} \\\n        --checkpoint-dir {CONFIG['checkpoint_dir']} \\\n        --data-dir {CONFIG['data_dir']} \\\n        --device {CONFIG['device']} \\\n        --resume auto\n\nelif TRAINING_MODE == \"distributed\":\n    # Run as distributed training worker\n    print(\"Starting distributed training worker...\")\n    print(\"This will:\")\n    print(\"  1. Connect to Redis queue\")\n    print(\"  2. Pull games generated by self-play workers\")\n    print(\"  3. Train neural network on GPU\")\n    print(\"  4. Publish trained models back to queue\")\n    print(\"\\nPress Ctrl+C to stop\\n\")\n    \n    !python scripts/distributed_training_worker.py \\\n        --redis-url \"{CONFIG['redis_url']}\" \\\n        --model-preset {CONFIG['model_preset']} \\\n        --batch-size {CONFIG['batch_size']} \\\n        --device {CONFIG['device']} \\\n        --min-games-for-training {CONFIG['min_batches_for_training']} \\\n        --publish-frequency {CONFIG['publish_frequency']} \\\n        --checkpoint-dir {CONFIG['checkpoint_dir']} \\\n        --lr {CONFIG['lr']} \\\n        --min-lr {CONFIG['min_lr']}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Monitor Progress\n\n### For Single-machine Mode:\nMonitor training metrics from checkpoint directory\n\n### For Distributed Mode:\nMonitor queue status and training worker progress"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if TRAINING_MODE == \"distributed\":\n    # Monitor distributed queue\n    print(\"Checking Redis queue status...\\n\")\n    !python scripts/monitor_queue.py --redis-url \"{CONFIG['redis_url']}\" --once\n    \nelse:\n    # Monitor single-machine training\n    import pandas as pd\n    import matplotlib.pyplot as plt\n\n    metrics_path = f\"{CONFIG['checkpoint_dir']}/training_metrics.csv\"\n\n    if os.path.exists(metrics_path):\n        df = pd.read_csv(metrics_path)\n        \n        # Plot metrics\n        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n        \n        df.plot(x='epoch', y='loss', ax=axes[0,0], title='Training Loss', grid=True)\n        df.plot(x='epoch', y='policy_acc', ax=axes[0,1], title='Policy Accuracy', grid=True)\n        df.plot(x='epoch', y='value_mae', ax=axes[1,0], title='Value MAE', grid=True)\n        df.plot(x='epoch', y='lr', ax=axes[1,1], title='Learning Rate', grid=True)\n        \n        plt.tight_layout()\n        plt.show()\n        \n        # Print summary\n        print(f\"\\nTraining Progress: {len(df)}/200 epochs\")\n        print(f\"\\nLatest metrics (last 5 epochs):\")\n        print(df.tail())\n        \n        # Estimate time remaining\n        if 'epoch_time' in df.columns and len(df) > 0:\n            avg_time = df['epoch_time'].mean()\n            remaining_epochs = 200 - len(df)\n            remaining_hours = (avg_time * remaining_epochs) / 3600\n            print(f\"\\n‚è±Ô∏è  Estimated time remaining: {remaining_hours:.1f} hours\")\n    else:\n        print(\"No metrics file found yet. Training hasn't started.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Check Latest Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# List all checkpoints\n",
    "checkpoints = sorted(glob.glob(f\"{CONFIG['checkpoint_dir']}/model_epoch_*.pt\"))\n",
    "\n",
    "if checkpoints:\n",
    "    print(f\"Found {len(checkpoints)} checkpoints:\\n\")\n",
    "    \n",
    "    # Show last 5\n",
    "    for cp in checkpoints[-5:]:\n",
    "        size_mb = os.path.getsize(cp) / 1024**2\n",
    "        epoch = cp.split('_')[-1].replace('.pt', '')\n",
    "        print(f\"  Epoch {epoch:>3}: {size_mb:>6.1f} MB - {os.path.basename(cp)}\")\n",
    "    \n",
    "    latest = checkpoints[-1]\n",
    "    print(f\"\\n‚úì Latest checkpoint: {os.path.basename(latest)}\")\n",
    "else:\n",
    "    print(\"No checkpoints found yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Download Checkpoint (Optional)\n",
    "\n",
    "For Colab/Kaggle, download to local machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only works in Colab\n",
    "if IS_COLAB:\n",
    "    from google.colab import files\n",
    "    \n",
    "    checkpoints = sorted(glob.glob(f\"{CONFIG['checkpoint_dir']}/model_epoch_*.pt\"))\n",
    "    if checkpoints:\n",
    "        latest = checkpoints[-1]\n",
    "        print(f\"Downloading: {os.path.basename(latest)}\")\n",
    "        files.download(latest)\n",
    "    else:\n",
    "        print(\"No checkpoints to download\")\n",
    "else:\n",
    "    print(\"Download not needed - checkpoints are already on VM storage\")\n",
    "    print(f\"Checkpoint location: {CONFIG['checkpoint_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Tips by Platform and Mode\n\n### Google Colab\n- ‚úì Sessions timeout after 12-24 hours\n- ‚úì Checkpoints saved to Google Drive persist\n- ‚úì Just re-run training cell to resume\n- ‚úì Use Colab Pro for A100 access\n- ‚úì **Perfect for distributed training worker** (free T4 GPU)\n\n### Vast.ai\n- ‚úì Hourly billing - pause anytime\n- ‚úì Use persistent storage for checkpoints\n- ‚úì Can rent spot instances (cheaper)\n- ‚ö†Ô∏è Spot instances can be interrupted\n\n### RunPod / Lambda Labs\n- ‚úì More reliable than spot instances\n- ‚úì Good for long training runs\n- ‚úì Usually have persistent storage\n\n### Local / AWS / GCP\n- ‚úì Full control over environment\n- ‚úì Can run indefinitely\n- ‚úì May need to install dependencies\n\n---\n\n## Distributed Training Architecture\n\nWhen using **distributed mode**, the architecture is:\n\n```\nMac M1 Pro (Self-Play)  ‚Üí  Redis Queue  ‚Üí  Colab T4 (Training)\n                              ‚Üì                    ‚Üì\n                        Stores games        Trains on GPU\n                              ‚Üë                    ‚Üì\n                        Latest model  ‚Üê  Publishes model\n```\n\n**How it works:**\n1. **Self-play workers** (Mac M1 Pro, CPU): Generate games via MCTS, push to queue\n2. **Redis queue** (REDIS_DOMAIN): Buffers games and models\n3. **Training worker** (This Colab notebook, GPU): Pulls games, trains NN, publishes model\n\n**Benefits:**\n- ‚úÖ Continuous training (GPU never idles)\n- ‚úÖ Use free Colab GPU + local CPU efficiently\n- ‚úÖ Scales to multiple self-play workers\n- ‚úÖ CPU and GPU work in parallel (4-6x faster than single-machine)\n\n**Cost:**\n- Redis VM: $0/month (minimal resources, 2GB RAM sufficient)\n- Colab: $0/month (free tier T4) or $10/month (Colab Pro for A100)\n- Total: **$0-10/month** vs $60-120 for dedicated cloud GPU\n\n---\n\n## Expected Training Time\n\n### Single-machine Mode (200 epochs)\n\n| GPU | VRAM | Batch | Time/Epoch | Total |\n|-----|------|-------|-----------|-------|\n| RTX 4090 | 24GB | 1024 | ~20-30 min | ~3-5 days |\n| A100 | 40GB | 2048 | ~15-20 min | ~2-3 days |\n| V100 | 16GB | 1024 | ~35-45 min | ~4-6 days |\n| T4 | 16GB | 512 | ~60-90 min | ~8-12 days |\n\n### Distributed Mode (Continuous Training)\n\n| Self-Play Workers | Training GPU | Games/Hour | Training Rate | Effective Speed |\n|-------------------|--------------|------------|---------------|-----------------|\n| 6 CPU (Mac M1) | Colab T4 | ~120-180 | 600-800 games/hr | 4-6x faster |\n| 1 MPS (Mac M1) | Colab T4 | ~40-50 | 600-800 games/hr | Balanced |\n| 6 CPU (Mac M1) | Colab A100 | ~120-180 | 1500-2000 games/hr | 8-10x faster |\n\n**Note**: GPU processes games faster than generation, so training is continuous (80-95% GPU utilization vs 25-40% single-machine).\n\n---\n\n## Troubleshooting\n\n### Single-machine Mode\n\n**Out of Memory:**\n- Script should auto-configure, but if OOM still happens:\n- Reduce workers: `--parallel-workers 2`\n- Reduce games: `--selfplay-games 100`\n\n**Slow Training:**\n- Check GPU is being used (should see CUDA in logs)\n- Reduce simulations if needed: `--mcts-simulations 100`\n\n**Connection Lost (Colab/Vast.ai):**\n- Just re-run training cell with `--resume auto`\n- Checkpoints saved every epoch\n\n### Distributed Mode\n\n**\"Connection refused\" to Redis:**\n- Check REDIS_URL is correct\n- Verify Redis server is running: `redis-cli -u <REDIS_URL> ping`\n- Check firewall allows port 6379\n\n**Training worker idle (no games):**\n- Check self-play workers are running\n- Monitor queue: `python scripts/monitor_queue.py --redis-url <URL>`\n- Expected: 10+ games in queue\n\n**Self-play workers not fetching new model:**\n- Workers fetch every 10 games (normal)\n- Check model published: Queue shows \"Latest model: epoch X\"\n\n**GPU utilization low in distributed mode:**\n- Should be 80-95% during training batches\n- If low, increase `--batch-size` or `--min-games-for-training`\n\n---\n\n## Getting Started with Distributed Training\n\n### Step 1: Deploy Redis Queue\n```bash\n# On your VM\ngit clone https://github.com/cheshir/alphagomoku.git\ncd alphagomoku\ndocker-compose up -d\n\n# Set REDIS_PASSWORD in environment\n# Map to REDIS_DOMAIN\n```\n\n### Step 2: Start Self-Play Workers (Mac M1 Pro)\n```bash\n# On your Mac\nexport REDIS_URL=\"redis://:YOUR_PASSWORD@REDIS_DOMAIN:6379/0\"\n\n# Start 6 CPU workers (recommended for large models)\nmake distributed-selfplay-cpu-workers\n\n# Or start 1 MPS worker (faster per-game, sequential)\n# make distributed-selfplay-mps-worker\n```\n\n### Step 3: Start Training Worker (This Colab Notebook)\n```python\n# In cell 2 above:\nTRAINING_MODE = \"distributed\"\nREDIS_URL = \"redis://:YOUR_PASSWORD@REDIS_DOMAIN:6379/0\"\n\n# Then run cells 3-7 to start training worker\n```\n\n### Step 4: Monitor Progress\n```python\n# Re-run cell 8 periodically to check queue status\n# Or run on your Mac:\npython scripts/monitor_queue.py --redis-url $REDIS_URL\n```\n\n---\n\n## Code Updates\n\nTo get latest code changes:\n```bash\n%cd {WORK_DIR}\n!git pull origin master\n\n# If you have local changes, stash first:\n# !git stash && git pull && git stash pop\n```"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}