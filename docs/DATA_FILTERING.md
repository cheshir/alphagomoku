# Training Data Filtering - The Missing Piece! ğŸ¯

## Your Critical Question

> "Are we sure the model will see winning sequences and forget about stupid moves?"

**Answer:** Not without data filtering! You identified a major gap.

---

## The Problem

### What We Had:
1. âœ… Proximity penalty in pattern features
2. âœ… Temperature scheduler
3. âœ… 30% tactical augmentation
4. âœ… TSS for forced sequences

### What Was Missing:
âŒ **No filtering of stupid moves from training data!**

### Why This Matters:
```
Self-play with MCTS â†’ Explores random moves (including stupid ones)
     â†“
Training data = MCTS visit counts (includes exploration)
     â†“
Model learns â†’ Mimics MCTS exploration (including stupid moves!)
```

**Even with all improvements, if stupid moves are in training data, model learns them.**

---

## The Solution: Data Filtering (Phase 5)

### New Component: `alphagomoku/train/data_filter.py`

Filters out training examples with:

1. **Edge moves in early game** (moves 0-10)
   - No playing on board borders before move 10
   - Prevents learning bad opening habits

2. **Distant moves** (>3 cells from nearest stone after move 5)
   - Enforces proximity principle
   - Prevents random moves far from action

3. **Tactical blunders**
   - If pattern channel shows strong threat (>0.8) elsewhere
   - But move played has weak pattern value (<0.3)
   - This is likely ignoring a critical threat â†’ filter it out

4. **Low confidence moves** (max policy probability <0.15)
   - If MCTS is very uncertain (nearly uniform policy)
   - Training signal is weak â†’ better to exclude

---

## How It Works

### Before Filtering:
```python
Self-play: 10,000 positions
Tactical aug: 3,000 positions
Total: 13,000 positions

# Includes stupid moves!
```

### After Filtering:
```python
Self-play: 10,000 positions
  â†“ Filter stupid moves (-10-20%)
  â†“ Filter low confidence (-5-10%)
Self-play filtered: ~7,500 positions

Tactical aug: 3,000 positions (kept - already high quality)

Total: ~10,500 HIGH-QUALITY positions
```

### Impact on Data Composition:
```
BEFORE filtering:
  Self-play: 77% (includes bad moves)
  Tactical: 23%

AFTER filtering:
  Self-play: 71% (bad moves removed)
  Tactical: 29%

Quality improvement: SIGNIFICANT!
```

---

## Expected Filtering Rates

### Early Training (Epochs 0-30):
- **Stupid moves removed:** 15-25%
- **Low confidence removed:** 10-15%
- **Total kept:** ~70%

**Why high?** Weak model â†’ bad self-play â†’ many stupid moves

### Mid Training (Epochs 31-100):
- **Stupid moves removed:** 5-10%
- **Low confidence removed:** 3-5%
- **Total kept:** ~90%

**Why better?** Stronger model â†’ better self-play â†’ fewer stupid moves

### Late Training (Epochs 100+):
- **Stupid moves removed:** 1-3%
- **Low confidence removed:** 1-2%
- **Total kept:** ~97%

**Why best?** Strong model â†’ high-quality self-play â†’ almost no filtering needed

---

## Integration

### In `scripts/train.py`:

```python
# After tactical augmentation
selfplay_data = augment_with_tactical_data(selfplay_data, ...)

# NEW: Filter stupid moves (Phase 5)
selfplay_data = apply_all_filters(selfplay_data, board_size=15)
```

### What You'll See During Training:

```
ğŸ” Filtering training data (starting with 13,000 examples)...

ğŸ“Š Data filtering: Kept 10,892/13,000 (83.8%)
   âŒ Removed 1,284 edge moves in early game
   âŒ Removed 567 distant moves (>3 cells away)
   âŒ Removed 257 tactical blunders

ğŸ“Š Low-confidence filtering: Kept 10,234/10,892 (94.0%)
   âŒ Removed 658 low-confidence examples (max_prob < 0.15)

âœ… Final dataset: 10,234 high-quality examples
```

---

## Why This Is Critical

### Without Filtering:
```
Epoch 0: Model random â†’ bad self-play â†’ learns bad moves
Epoch 20: Model weak â†’ still bad self-play â†’ reinforces bad moves
Epoch 50: Model OK â†’ but has learned bad habits â†’ hard to unlearn
Epoch 100: Model stuck with bad habits
```

### With Filtering:
```
Epoch 0: Model random â†’ bad self-play â†’ FILTERED â†’ learns only good moves
Epoch 20: Model improving â†’ better self-play â†’ less filtering needed
Epoch 50: Model good â†’ high-quality self-play â†’ minimal filtering
Epoch 100: Model strong â†’ excellent play â†’ nearly no filtering
```

---

## Comparison

| Aspect | Without Filtering | With Filtering |
|--------|------------------|----------------|
| **Edge moves learned?** | Yes (bad habit) | No (filtered out) |
| **Distant moves learned?** | Yes (bad habit) | No (filtered out) |
| **Tactical blunders learned?** | Yes (very bad!) | No (filtered out) |
| **Data quality** | Mixed | High |
| **Learning speed** | Slow (noise) | Fast (clean signal) |
| **Final strength** | Limited | Much stronger |

---

## Testing

Test that filtering works:

```bash
python -c "
from alphagomoku.train.data_filter import filter_stupid_moves
from alphagomoku.selfplay.selfplay import SelfPlayData
import numpy as np

# Create fake example with edge move
board = np.zeros((15, 15), dtype=np.int8)
board[7, 7] = 1  # One stone in center

# Create state with edge move
state = np.zeros((5, 15, 15), dtype=np.float32)
state[0, 7, 7] = 1.0  # Own stone

# Policy that picks edge move (BAD!)
policy = np.zeros(225, dtype=np.float32)
policy[0] = 1.0  # Top-left corner (edge)

example = SelfPlayData(state=state, policy=policy, value=0.0)

# Filter
filtered = filter_stupid_moves([example], board_size=15)

if len(filtered) == 0:
    print('âœ… Edge move correctly filtered out!')
else:
    print('âŒ Edge move NOT filtered (bug!)')
"
```

---

## Summary

### What This Adds:

âœ… **Explicit filtering of stupid moves**
- Edge moves in early game
- Distant moves (>3 cells)
- Tactical blunders
- Low confidence moves

âœ… **Progressive improvement**
- Early: Filters 20-30% (weak model)
- Mid: Filters 5-10% (better model)
- Late: Filters 1-3% (strong model)

âœ… **Clean training signal**
- Model learns ONLY from good moves
- No reinforcement of bad habits
- Faster convergence to strong play

### Combined with Previous Improvements:

1. Speed optimizations (3-5x faster)
2. Proximity penalty (guides away from bad moves)
3. Temperature scheduler (cleaner exploration)
4. Tactical augmentation (30% forced moves)
5. **Data filtering (removes stupid moves)** â† NEW!

### Result:

**Strong model that learns ONLY good moves, forgets bad habits, and plays intelligently!**

---

## Ready to Train!

The complete pipeline now ensures:

1. **Self-play** generates data (some good, some bad)
2. **Tactical augmentation** adds forced-move examples
3. **Data filtering** removes stupid moves
4. **Model training** learns ONLY from high-quality data
5. **Result**: Strong player without bad habits!

Run training with:
```bash
make train
```

This will now produce a **significantly smarter** model! ğŸ‰
